\chapter{Related works}\label{chap:related_works}

 A~survey paper by Pajarola et al.~\cite{survey} summarizes the~best known multiresolution terrain rendering methods. All these methods also handle the~rendering supported by their own LOD-ing hierarchies. However, some of them contain data compression as a sub-task. Some of them are designed to render just a~flat area, others are able to render the~whole planet. In the~rest of this section, we will briefly describe the~methods which contain terrain data compression
 
 The~first such methods we came accross are C-BDAM\footnote{Compressed Batched Dynamic Adaptive Meshes}~\cite{cbdam} and P-BDAM\footnote{Planet Sized Batched Dynamic Adaptive Meshes}~\cite{pbdam} by  Gobbetti et al. After comparing them to the~other related works, they turned out to be the~most promising ones. Both these methods handle the~LOD rendering too and perform wavelet-based~\cite{waveletsTutorial} data compression in the~refinement of a~node of their~LOD hierarchy. Once the~values of a~certain node are known, they are used to predict the~values of its children as accurately as possible. After that, the~differences between these~predictions and the~real values are computed. These are called residuals. With the~help of them, the~real values can be restored with absolute accuracy. However, the~residuals are then quantized to achieve better compression ratio which means that the~compression is lossy. Then, they are losslessly compressed by an~entropy codec~\cite{entropy1, entropy2}. Both these methods are able to compute the~residuals in the~way which ensures that the~error of the~reconstructed data is kept within a~maximum error bound adjustable by the~user in every node of their LOD hierarchy. This can be achieved by a~slight modification of the~second-generation wavelet lifting scheme~\cite{two-stage}. C-BDAM is designed to render just a~flat portion of terrain, whereas P-BDAM is just C-BDAM modified to be able to render a~whole planet. These modifications do not include any improvements to the~efficiency of the~compression, so, from our point of view, it is suffiecient to know just C-BDAM from these two methods. What makes C-BDAM the~most interesting are two aspects: the~outstanding compression ratio achieved and the~ability to respect a~certain user-set maximum per-sample error bound (the~ratio of 64:1 on the~whole planet with 16m maximum deviation).
 
 Wavelet~tiled pyramids by Ricardo Olanda~\cite{jpeg2000terrain} is another method for rendering a~flat portion of terrain. This method contains data compression based on the~same principle - the~residuals needed to reconstruct the~children of a~square node of the~terrain LOD hierarchy are compressed. The~computation of residuals is based on the~wavelet-based JPEG2000 standard. This method is not able to reconstruct the~data within a~certain maximum-error bound which makes it almost not interesting to us at all. Due to this fact, we cannot judge its compression ratios, as they are not connected with any maximum-error bound. Besides, the~visual artifacts between adjacent nodes of different LODs are not handled by its rendering pipeline, but it needs not bother us anyway.
 
 Apart from that, we found several methods dedicated to compressing triangulated terrain representations~\cite{meshesGPU1, meshesGPU2, meshes}. Two of them~\cite{meshesGPU1, meshesGPU2} perform decompression on GPU and are also able to keep the~compressed data within a~certain maximum deviation adjustable by the~user. However, they are all more complicated than the~previously mentioned ones which stems from the~greater complexity and irregularity of their input data. Unlike the~previous methods which work with regularly distributed height samples, the~triangle meshes these methods compress can be irregular which introduces the~need to handle more spatial information. Due to this, the~compression ratios they achieve are all way lower than the~ones of the~previous methods. Our method is expected to work with just regularly distributed height samples - square mip-maps, so we see no sense in getting inspired by compression methods which have to handle much more complicated data which results in worse efficiency and greater complexity. This is why these methods are not as interesting to us.
 
 HFPaC~\cite{fieldGPU} by Durdevic and Tartalja is the~only method we found which focuses solely on heightmap compression, which looked promising at first. Moreover, it is GPU-based which can bring greater performance. However, it is not completely suitable for our purpose for several reasons. It is near-lossless, but the~maximum deviation of the~reconstructed data cannot be controlled by the~user. Quite to the~contrary, it varies on the~basis of both the~characteristics of the~input and the~settings of its internal parameters, such as the~size of its compression block. Even though the~authors of the~method observed that this deviation is relatively small most of the~time (up to 2~meters), this is undoubtedly not exactly what we want. In addition, this method is only able to provide exactly three layers of resolution of the~compressed data, but our method should be able to provide a~larger number of mip-maps which varies depending on the~dimension of the~input. We would have a~hard time trying to tailor this method to our needs, because it is really complicated. Additional modifications made to it could result in its even greater complexity and possible lack of efficiency compared to the~other simpler available methods which are already able to respect a~maximum deviation adjustable by the~user and to perform multi-level progressive decompression. All these factors caused this method to become not interesting to us.
 
 We also read a~survey about fractal compression~\cite{fractal} by Saupe et al., because we saw a~tiny opportunity to use this kind of approach in our thesis. However, such methods are quite complicated and designed to compress the~whole data at once without any possibility of progressive decompression. More precisely, it might be possible to introduce the~progression there, but to our best knowledge, no one has found the~way yet and we did not see any straightforward option to do this either. Additionally, the~aspect of data this family of methods benefits from the~most is its regularity or at least certain repeating. This can be best exploited when compressing huge amounts of data. However, our method should be able to compress and decompress just one small square of height samples completely independently from the~others and we cannot benefit from much repetition on such a~small area. All in all, the~path of fractal compression seemed very hard to follow, so we decided to rather stick to the~more promising options - the~methods which support multiresolution approach by definition.
 
 After summarizing the~available literature, we decided get inspired by the~compression inside C-BDAM and tailor it for our needs to create our own method. The~main reasons are that C-BDAM achieves the~best compression ratios and is able to respect a~certain maximum-error bound adjustable by the~user which is exactly what we needed, too. As we already mentioned in the~beginning, the~size of the~compressed data was the~most important constraint, which is why we chose to follow the~best compression ratio. However, at the~same time, it did not seem that C-BDAM sacrificed a~lot in order to achieve the~good ratios - its compression is relatively simple, maybe the~simplest from among the~studied methods, which makes it easy to understand and also possibly fast when implemented. Thus, it remained to modify the~compression inside C-BDAM to be usable in our method. This did not seem to be too hard to do, as C-BDAM performs the~compression inside the~refinement of a~node of its LOD hierarchy. Due to the~request that our method should be able to decompress the~mip-maps progressively, it seemed convenient to put the~compression in our method inside the~refiniment of a~mip-map. This way, only the~residuals needed to reconstruct the~following finer mip-map would have to be fetched in order to perform a~single step of refinement. So, instead of a~LOD node in C-BDAM, we put a~mip-map in our method.  Additionally, we significantly simplified the~compression equations performed in C-BDAM in order to increase the~efficiency and speed of our method, while still being able to satisfy the~required maximum absolute error bound constraint. However, let it be repeated again that a~LOD node in C-BDAM is actually the~basic component of its multiresolution rendering hierarchy, whereas a~mip-map in our method not, it is only a~side component of the~LOD hierarchy of the~application into which our method should be plugged. The~main component of the~multiresolution rendering hierarchy of this application is the~square which should be compressed by this method, so our method lies outside this hierarchy. However, it is not hard to utilize the~compression in C-BDAM inside our method, because the~principle remains the~same in both cases - we refine a~set of height samples to a~more detailed one.
