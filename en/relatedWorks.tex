\chapter{Related works}\label{chap:related_works}

 A~survey paper by Pajarola et al.~\cite{survey} summarizes the~best known multiresolution terrain rendering methods. All these methods also handle the~rendering supported by their own LOD-ing hierarchies. We only looked at those of them which contain data compression as a~sub-task. Some of them are designed to render just a~flat area~\cite{cbdam, jpeg2000terrain, meshesGPU1, meshesGPU2}, others are able to render the~whole planet~\cite{pbdam, meshes}. In the~rest of this chapter, we will briefly describe the~methods which contain terrain data compression
 
 C-BDAM\footnote{Compressed Batched Dynamic Adaptive Meshes}~\cite{cbdam} and P-BDAM\footnote{Planet Sized Batched Dynamic Adaptive Meshes}~\cite{pbdam} by Gobbetti et al. turned out to be the~most promising ones after comparing with the~rest of the~methods. Both these methods handle the~LOD rendering too and perform wavelet-based~\cite{waveletsTutorial} data compression in the~refinement of a~node of their~LOD hierarchy. Once the~values of a~certain node are known, they are used to predict the~values of its children as accurately as possible. After that, the~differences between these~predictions and the~real values are computed. These are called residuals. With the~help of them, the~real values can be restored with absolute accuracy. However, the~residuals are then quantized to achieve better compression ratio which means that the~compression is lossy. Then, they are losslessly compressed by an~entropy codec~\cite{entropy1, entropy2}. Both these methods are able to compute the~residuals in the~way which ensures that the~error of the~reconstructed data is kept within a~maximum error bound adjustable by the~user in every node of their LOD hierarchy. This can be achieved by a~slight modification of the~second-generation wavelet lifting scheme~\cite{two-stage}. C-BDAM is designed to render just a~flat portion of terrain, whereas P-BDAM is just C-BDAM modified to be able to render a~whole planet. These modifications do not include any improvements to the~efficiency of the~compression, so, from our point of view, it is suffiecient to know just C-BDAM from these two methods. What makes C-BDAM the~most interesting are two aspects: the~outstanding compression ratio achieved and the~ability to respect a~certain user-set maximum per-sample error bound (the~ratio of 64:1 on the~whole planet - 58GB with 90m resolution - with 16m maximum height deviation).
 
 Wavelet~tiled pyramids by Ricardo Olanda~\cite{jpeg2000terrain} is another method for rendering a~flat portion of terrain. This method contains data compression based on the~same principle - the~residuals needed to reconstruct the~children of a~square node of the~terrain LOD hierarchy are compressed. The~computation of residuals is based on the~wavelet-based JPEG2000 standard. This method is not able to reconstruct the~data within a~certain maximum-error bound which makes it almost not interesting to us at all. Due to this fact, we cannot judge its compression ratios, as they are not connected with any maximum-error bound. Besides, the~visual artifacts between adjacent nodes of different LODs are not handled by its rendering pipeline, but it is not our concern anyway.
 
 Apart from that, we found several methods dedicated to compressing triangulated terrain representations~\cite{meshesGPU1, meshesGPU2, meshes}. Two of them~\cite{meshesGPU1, meshesGPU2} perform decompression on GPU and are also able to keep the~compressed data within a~certain maximum deviation adjustable by the~user. However, they add more complexity to the~task as they allow for greater irregularity of their input data. Unlike the~previous methods which work with regularly distributed height samples, the~triangle meshes these methods compress can be irregular which introduces the~need to handle more spatial information. Due to this, the~compression ratios they achieve are all way lower than the~ones of the~previous methods. Our method is expected to work with just regularly distributed height samples - square mip-maps, so we do not see as much potential in getting inspired by compression methods which have to handle much more complicated data which results in worse efficiency and greater complexity.
 
 HFPaC~\cite{fieldGPU} by Durdevic and Tartalja is the~only method we found which focuses solely on heightmap compression, which looked promising at first. Moreover, it is GPU-based which can bring greater performance. However, it has several severe limitations. It is near-lossless, but the~maximum deviation of the~reconstructed data cannot be controlled by the~user. Quite to the~contrary, it varies on the~basis of both the~characteristics of the~input and the~settings of its internal parameters, such as the~size of its compression block. Even though the~authors of the~method observed that this deviation is relatively small most of the~time (up to 2~meters), this is undoubtedly not exactly what we want, as our method should be able to support any arbitrary maximum error set by the~user. In addition, this method is only able to provide exactly three layers of resolution of the~compressed data, but our method should be able to provide a~larger number of mip-maps which varies depending on the~dimension of the~input. It would not be easy to tailor this method to our needs, because it is really complicated. Additional modifications made to it could result in its even greater complexity and possible lack of efficiency compared to the~other simpler available methods which are already able to respect a~maximum deviation adjustable by the~user and to perform multi-level progressive decompression.
 
 Because terrain exhibits some fractal characteristics~\cite{fractalChar}, we also read a~survey about fractal compression~\cite{fractal} by Saupe et al. However, such methods are quite complicated and designed to compress the~whole data at once without any possibility of progressive decompression. More precisely, it might be possible to introduce the~progression there, but to our best knowledge, no one has found the~way yet and we did not see any straightforward option to do this either. Additionally, the~aspect of data this family of methods benefits from the~most is its regularity or at least certain repeating. This can be best exploited when compressing huge amounts of data. However, our method should be able to compress and decompress just one small square of height samples completely independently from the~others and we cannot benefit from much repetition on such a~small area. All in all, with respect to the~planned usage of the~method, the~path of fractal compression seemed not very perspective, but it might be interesting for further research. 
 
 At the~end, we decided to rather stick to the~more promising options - the~methods which support multiresolution approach by definition. When designing our method, we decided to start off the ideas behind the~compression inside C-BDAM. The~main reasons are that C-BDAM achieves the~best compression ratios and is able to respect a~certain maximum-error bound adjustable by the~user which is exactly what we needed, too. As we already mentioned in the~beginning, the~size of the~compressed data was the~most important constraint, which is why we chose to follow the~best compression ratio. The~compression inside C-BDAM is relatively simple which makes it easy to understand and also fast when implemented. However, C-BDAM contains features which we did not need, so we omitted them and just isolated the~compression and tailored it for our needs. 
 
 C-BDAM performs the~compression inside the~refinement of a~node of its LOD hierarchy. Due to the~request that our method should be able to decompress the~mip-maps progressively, it seemed convenient to put the~compression in our method inside the~refiniment of a~mip-map. This way, only the~residuals needed to reconstruct the~following finer mip-map would have to be fetched in order to perform a~single step of refinement. So, instead of a~LOD node in C-BDAM, we put a~mip-map in our method.  Additionally, we significantly simplified the~compression equations performed in C-BDAM in order to increase the~efficiency and speed of our method, while still being able to satisfy the~required maximum absolute error bound constraint. However, let it be repeated again that a~LOD node in C-BDAM is actually the~basic component of its multiresolution rendering hierarchy, whereas a~mip-map in our method not, it is only a~side component of the~LOD hierarchy of the~application into which our method should be plugged. The~main component of the~multiresolution rendering hierarchy of this application is the~square which should be compressed by this method, so our method lies outside this hierarchy. However, it is not hard to utilize the~compression in C-BDAM inside our method, because the~principle remains the~same in both cases - we refine a~set of height samples to a~more detailed one.
