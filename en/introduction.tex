\chapter{Introduction}\label{chap:introduction}

In the~beginning of this chapter, we clearly state the~aim of this thesis and then briefly describe and summarize the~works most related to the~topic. In the~end, we present and explain our decision how to solve the~assignment which we made on the~basis of this literature.

The~aim of this thesis is to either find or come up with a~method which is able to compress a~regular square of float terrain samples as efficiently as possible, while enabling subsequent real-time progressive decompression of its data from the~coarsest to the~finest mip-map. The~decompression should be as fast as possible. We should explore the~possibility to perform the~decompression on GPU mainly in order to save RAM. We should keep in mind that from among the~mentioned optimization objectives, the~resulting size of compressed data is the~most important one. The~maximum per-sample deviation of the~compressed data must be controllable by the~user. No rendering of data has to be handled, as it is supposed that the~application using this method will handle this. Knowing these requirements, we started researching the~available literature. However, we did not find any method which solves exactly this task while doing nothing additional which would only decrease the~required efficiency for our purpose. The~point is that most works related to heightmap compression are also able to render the~compressed data. Many times, the~compression is built into their multiresolution (LOD-ing\footnote{LOD is the~abbreviation of level of detail - degradation of quality of the~displayed data with the~growing distance in order to optimize the~rendering}) rendering pipeline. Other times, the~works pay attention to compressing a~multiresolution hierarchy already prepared for rendering. Of course, for a~terrain renderer, it is crucial to implement some form of multiresolution rendering in order to reach reasonable frame rates, but this is not what our method should handle. 

Our method should just be plugged into an~already existing multiresolution rendering engine, the~node of the~LOD hierarchy of which is a~regular square of float height samples stored completely independently from the~other nodes. It must be stated that our method should not interfere in any way with how this engine traverses this hierarchy in order to render a~scene. The~mip-maps which the~decompression should be able to provide are in fact a~form of level of detail technique too, but it must be made clear that the~ability to provide simplified multiresolution representations of every square of the~LOD hierarchy is just a~tiny part of the~multiresolution rendering pipeline, a~LODing terrain engine can certainly not be build solely on this ability. The~application using the~desired method should be able to traverse its multiresolution hierarchy of square nodes on itself in order to render a~scene. After selecting which squares should be displayed (the~lower the~distance, the~more detailed the~displayed square), the~application then should decide for each of the~squares which mip-map of it will be displayed. Thus, the~mip-maps present a~less significant LOD concept inside the~greater LOD concept - the~multiresolution squares hierarchy. The~mip-map selection can be based on the~screen-space area of the~square in order to reduce the~terrain aliasing. For example, when looking at a~certain terrain square from a~side, a~coarser mip-map of it should be chosen than in case we look at it from the~top.

As we already stated, we did not find any paper which solves exactly our assignment. Many methods are able to compress multiresolution hierarchies prepared to be used in rendering which introduces unnecessary overhead for us. Thus, to find out how the~terrain height data can be compressed while respecting a~maximum-error bound constraint, we had to look for the~suitable compression inside the~methods the~scope of which is broader. We started with a~survey paper summarizing the~best known multiresolution terrain rendering methods~\cite{survey}. All these methods also handle the~rendering supported by their own LODing hierarchies. However, some of them contain data compression as a sub-task. Some of them are designed to render just a~flat area, others are able to render the~whole planet. For our purpose, the~methods rendering a~flat portion of terrain seemed sufficient to get to know, the~scope of both groups of methods is larger than required anyway, but we dug through all of them with the~aim to find the~most efficient compression inside them. We did not limit our search only to the~methods referenced by this survey paper, we also searched recursively by references from the~already discovered papers and, of course, on the~internet. In the~rest of this section, we will briefly describe the~methods which contain terrain data compression

The~first methods we came accross are C-BDAM\footnote{Compressed Batched Dynamic Adaptive Meshes}~\cite{cbdam} and P-BDAM\footnote{Planet Sized Batched Dynamic Adaptive Meshes}~\cite{pbdam}. Both these methods handle the~LOD rendering too and perform the~data compression in the~refinement of a~node of their~LOD hierarchy. Once the~values of a~certain node are known, they are used to predict the~values of its children as accurately as possible. After that, the~differences between these~predictions and the~real values are computed. These are called residuals. With the~help of them, the~real values can be restored with absolute accuracy. However, the~residuals are then quantized to achieve better compression ratio which means that the~compression is lossy. Then, they are losslessly compressed by an~entropy codec. Both these methods are able to compute the~residuals in the~way which ensures that the~error of the~reconstructed data is kept within a~maximum error bound adjustable by the~user in every node of their LOD hierarchy. This can be achieved by a~slight modification of the~second-generation wavelet lifting scheme~\cite{two-stage}. C-BDAM is designed to render just a~flat portion of terrain, whereas P-BDAM is just C-BDAM modified to be able to render a~whole planet. These modifications do not include any improvements to the~efficiency of the~compression, so, from our point of view, it is suffiecient to know just C-BDAM from these two methods. What makes C-BDAM the~most interesting are two aspects: the~outstanding compression ratio achieved and the~ability to respect a~certain user-set maximum per-sample error bound (the~ratio of 64:1 on the~whole planet with 16m maximum deviation).

Wavelet~tiled pyramids~\cite{jpeg2000terrain} is another method for rendering a~flat portion of terrain. This method contains data compression based on the~same principle - the~residuals needed to reconstruct the~children of a~square node of the~terrain LOD hierarchy are compressed. The~computation of residuals is based on the~wavelet-based JPEG2000 standard. This method is not able to reconstruct the~data within a~certain maximum-error bound which makes it almost not interesting to us at all. Due to this fact, we cannot judge its compression ratios, as they are not connected with any maximum-error bound. Besides, the~visual artifacts between adjacent nodes of different LODs are not handled by its rendering pipeline, but it needs not bother us anyway.

Apart from that, we found several methods dedicated to compressing triangulated terrain representations~\cite{meshesGPU1, meshesGPU2, meshes}. Two of them~\cite{meshesGPU1, meshesGPU2} perform decompression on GPU and are also able to keep the~compressed data within a~certain maximum deviation adjustable by the~user. However, they are all more complicated than the~previously mentioned ones which stems from the~greater complexity and irregularity of their input data. Unlike the~previous methods which work with regularly distributed height samples, the~triangle meshes these methods compress can be irregular which introduces the~need to handle more spatial information. Due to this, the~compression ratios they achieve are all way lower than the~ones of the~previous methods. Our method is expected to work with just regularly distributed height samples - square mip-maps, so we see no sense in getting inspired by compression methods which have to handle much more complicated data which results in worse efficiency and greater complexity. This is why these methods are not as interesting to us.

HFPaC~\cite{fieldGPU} is the~only method we found which focuses solely on heightmap compression, which looked promising at first. Moreover, it is GPU-based which can bring greater performance. However, it is not completely suitable for our purpose for several reasons. It is near-lossless, but the~maximum deviation of the~reconstructed data cannot be controlled by the~user. Quite to the~contrary, it varies on the~basis of both the~characteristics of the~input and the~settings of its internal parameters, such as the~size of its compression block. Even though the~authors of the~method observed that this deviation is relatively small most of the~time (up to 2~meters), this is undoubtedly not exactly what we want. In addition, this method is only able to provide exactly three layers of resolution of the~compressed data, but our method should be able to provide a~larger number of mip-maps which varies depending on the~dimension of the~input. We would have a~hard time trying to tailor this method to our needs, because it is really complicated. Additional modifications made to it could result in its even greater complexity and possible lack of efficiency compared to the~other simpler available methods which are already able to respect a~maximum deviation adjustable by the~user and to perform multi-level progressive decompression. All these factors caused this method to become not interesting to us.

We also read a~survey about fractal compression~\cite{fractal}, because we saw a~tiny opportunity to use this kind of approach in our thesis. However, such methods are quite complicated and designed to compress the~whole data at once without any possibility of progressive decompression. More precisely, it might be possible to introduce the~progression there, but to our best knowledge, no one has found the~way yet and we did not see any straightforward option to do this either. Additionally, the~aspect of data this family of methods benefits from the~most is its regularity or at least certain repeating. This can be best exploited when compressing huge amounts of data. However, our method should be able to compress and decompress just one small square of height samples completely independently from the~others and we cannot benefit from much repetition on such a~small area. All in all, the~path of fractal compression seemed very hard to follow, so we decided to rather stick to the~more promising options - the~methods which support multiresolution approach by definition.

After summarizing the~available literature, we decided get insipred by the~compression inside C-BDAM and tailor it for our needs to create our own method. The~main reasons are that C-BDAM achieves the~best compression ratios and is able to respect a~certain maximum-error bound adjustable by the~user which is exactly what we needed, too. As we already mentioned in the~beginning, the~size of the~compressed data was the~most important constraint, which is why we chose to follow the~best compression ratio. However, at the~same time, it did not seem that C-BDAM sacrificed a~lot in order to achieve the~good ratios - its compression is relatively simple, maybe the~simplest from among the~studied methods, which makes it easy to understand and also possibly fast when implemented. Thus, it remained to modify the~compression inside C-BDAM to be usable in our method. This did not seem to be too hard to do, as C-BDAM performs the~compression inside the~refinement of a~node of its LOD hierarchy. Due to the~request that our method should be able to decompress the~mip-maps progressively, it seemed convenient to put the~compression in our method inside the~refiniment of a~mip-map. This way, only the~residuals needed to reconstruct the~following finer mip-map would have to be fetched in order to perform a~single step of refinement. So, instead of a~LOD node in C-BDAM, we put a~mip-map in our method.  Additionally, we significantly simplified the~compression equations performed in C-BDAM in order to increase the~efficiency and speed of our method, while still being able to satisfy the~required maximum absolute error bound constraint. However, let it be repeated again that a~LOD node in C-BDAM is actually the~basic component of its multiresolution rendering hierarchy, whereas a~mip-map in our method not, it is only a~side component of the~LOD hierarchy of the~application into which our method should be plugged. The~main component of the~multiresolution rendering hierarchy of this application is the~square which should be compressed by this method, so our method lies outside this hierarchy. However, it is not hard to utilize the~compression in C-BDAM inside our method, because the~principle remains the~same in both cases - we refine a~set of height samples to a~more detailed one.

We also spent some time studying the~core component of compression in many of the~mentioned papers, including C-BDAM - wavelet transform - in order to possibly find an~improvement to it which might be beneficial for our method. You will be able to see the~outcomes of it in Chapter~\ref{chap:wavelets_comp}, where we briefly describe the~basic theory of wavelets and also possible !!!dopln!!!improvements to the~wavelet-based methods. At the~end of it, we compare C-BDAM and our method to these methods. In Chapter~\ref{chap:outline}, we briefly describe the~basic outline of the~method. In Chapter~\ref{chap:details}, we describe the~details of the~method. In Chapter~\ref{chap:cbdam_comp}, we compare the~core algorithm of this method to the~algorithm of C-BDAM. We present the~results in Chap~\ref{chap:results}. In Chapter~\ref{sec:conclusion}, we discuss these results and also the~possibility how the~decompression could be put onto GPU in order to spare RAM. So far, we have not found a~way how this could be done.
