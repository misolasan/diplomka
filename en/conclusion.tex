\chapter{Conclusion}\label{sec:conclusion}

The aim of this thesis was to design a~heightmap compression plugin into an existing real-time planet renderer. This plugin should be able to compress a~regular square block of height samples and progressively decompress it in the~real-time, from the~smallest mip-map to the~largest one. Apart from this, the plugin should not in any way interfere with the~rendering pipeline of the~application. After summarizing the~available literature, we decided to write our own method inspired by the~most promising compression approach used inside C-BDAM, the~method for rendering compressed flat terrain. Our method turned out to be suitable for the~purpose. The~decompression of one 256x256 square which is the~size of square used in the~application takes only about 1ms. The~compression of it takes about 30ms which is still bearable, as it does not have to be performed in the~real time. The~compression ratios of our method are comparable to the~ones of C-BDAM which provides the~best compression ratios among the~studied methods.

We should also investigate the~possibility to perform the~decompression on GPU in order to spare RAM. However, the~main priority was still to minimize the~size of the~compressed data which is why we followed the~compression ratio in the~first place. With respect to it, we designed our method and only after this we analyzed whether and how its decompression could be put onto GPU. We saw two places where the~decompression could already switch to GPU:
\begin{enumerate}
	\item{before the~decompression of residuals}
	\item{right after the decompression of residuals}
\end{enumerate}

The~first possibility would imply that the~decompression performed by Zlib would have to be put onto GPU. As we already described in Sec.~???, Zlib contains both entropy and dictionary compression, so this would be really complicated. At the~end, we did not have time to try to do it.

The~second possibility means less spared RAM, but still some, because, as we described in Sec.~\ref{sec:top-down}, before the~residuals are losslessly compressed they are packed - divided by their quantization interval, shifted, so they are all positive and then skewed to the~number of bits of the~largest value. So, right after their decompression by Zlib, they are still a~bit compressed. Theoreticcaly, their unpacking could be done on GPU. Then, the~subsequent reconstruction of a~larger mip-map from the~smaller one would have to be performed on GPU, too. The~ability to do this surely already depends on the~implementation of the~renderer. This is where we encountered a~problem.

The~renderer uses just one vertex buffer per each square. Then, it sends a~mip-map of the~square to GPU as a~texture and according to this heights in this texture, it performs displacement of vertices of the~buffer. Thus, if the~unpacking of residuals and the~progressive mip-maps reconstruction were all to be performed on GPU, we would no longer send any textures to GPU, but we would just send the~unpacked residuals there which is the~only remaining way how to spare RAM in our case. The~problem is that when constructing a~certain mip-map, we must have the~previous one at hand. However, in the~current implementation of the~renderer which does not use CUDA, we have no possibility to create and maintain an~array of values. The~second possibility would be to directly utilize the~information in vertices - their location. To determine the~height of a~certain vertex, we would have to average the~suitably backward transformed heights of its neighbors (Sec.~\ref{sec:top-down}). This would have to be done in vertex shader. However, this is impossible, as in vertex shader, we cannot access neighboring vertices of the~current vertex. Due to all these factors, we finally decided not to implement any part of decompression on GPU.