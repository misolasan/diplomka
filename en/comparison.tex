\chapter{Functional comparison to C-BDAM and wavelets}\label{chap:cbdam_comp}

As we already mentioned, C-BDAM contains the~whole rendering pipeline, whereas our method does not. However, it can be compared to C-BDAM in terms of how lifting is performed. As we already mentioned in the~end of Section~\ref{sec:wavelets}, C-BDAM omits a~half of the~samples while constructing a~coarser LOD, whereas our method omits three fourths of the~samples. This is spatially equivalent to two steps of lifting in C-BDAM (Fig.~\ref{fig:cbdam_lifting}). The first step removes the~pixels $b$ and the~second step removes the~pixels $c$ as seen in Fig.~\ref{fig:subst}. Nevertheless, this equality is only spatial. 

In our method, an analogy of the~update operator of lifting is used to construct $\lnorm{i}$ from $\lnorm{i+1}$ (the~averaging of four neighboring pixels - Sec.~\ref{sec:details}). However, the~lifting is not complete in our method as it does not contain the~prediction operator - no residuals are computed there yet. In C-BDAM, also a~prediction operator is used in the~lifting to produce intermediate residuals. However, using just these residuals would not guarantee any maximum error bound, so C-BDAM makes another top-bottom pass to correct the~residuals against the~real values of samples produced in the~first bottom-top pass. To make this correction fit into the~original wavelet framework, several intricate computations need to be performed, including division, which is quite a~large performance hit.

Our vision was that once it is needed to perform an extra~top-bottom pass to correct the~residuals so that the~maximum error bound is guaranteed, it is not neccessary to compute any temporary values of the~residuals during the~lifting steps (the~construction of the~LOD pyramid). This is why we perform just an~analogy of the~update (the~averaging of pixels) in the~update-first scheme and let the~following top-bottom pass compute suitable values of residuals. This is obviosly a~major deviation from the~wavelet scheme. In the top-bottom pass, we just predict the~values in the~finer LOD as accurately as possible, but these predictions have no linkage to the~previous bottom-top pass, as they have not been used there at all. Then we directly compute the~residuals with respect to the~original values computed in the~first bottom-top pass at the~corresponding levels.

All in all, it can be said that the~way the~residuals are computed in this method is an~extreme simplification of the~way they are computed in C-BDAM. This way of computation does not even conform to the~second-generation wavelet scheme - the~lifting is not complete and the~reconstruction is not the~inverse of lifting. We think that without the~residuals quantization or the~per-level correction of residuals, respecting the~wavelet scheme makes sense, as it ensures computational equivalency with the~first-generation wavelets. However, in case the residuals need to be corrected at each level, we think that conforming to a~wavelet scheme makes no sense, because this correction immediatelly destroys the~mentioned equivalence - once a~residual is cropped in order to get the~resulting value closer to the~actual data, it cannot be said that any of the~following reconstruction is the~inverse to the~lifting performed before. Moreover, thanks to the~mentioned deviation of C-BDAM from the~classical update-first second-generation wavelet discussed in Section~\ref{sec:wavelets}, we question its computational equivalency with the~first-generation wavelet even with no residuals quantization or cropping performed. Because of this, we think that the~computations made in the~second top-bottom pass can be optimized this way without any cost. Thus, this method would probably better be called wavelet-inspired than wavelet-based.