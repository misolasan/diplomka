\chapter{Functional comparison to C-BDAM and wavelets}\label{chap:cbdam_comp}

In this chapter, with the~details of this method described, we will compare it with C-BDAM in more detail. The~main difference which we already mentioned is that unlike in our method, in C-BDAM, the~whole rendering pipeline is contained. However, we can compare how lifting is performed in these two methods. From the~point of view of lifting, a~mip-map level in our method is analogic to a~node of LOD hierarchy of C-BDAM and the~per-mip-map maximum absolute error bound constraint which our method has to keep is analogic to the~same per-LOD-node constraint in C-BDAM. In the~end of Section~\ref{sec:wavelets_cbdam}, we already mentioned that when constructing a~coarser LOD node from the~finer one, C-BDAM omits a~half of the~samples of the~finer node. To the~contrary, our method omits three fourths of the~samples of the~finer mip-map level when constructing a~coarser one. With respect to the~spatial arrangment of the~samples, this transition is equivalent to two fine-to-coarser transitions in C-BDAM ???(nieco viac)(Fig.~\ref{fig:cbdam_lifting}). In the first transition, the~pixels labeled as $b$ are removed and in the~second one, the~pixels labeled as $c$ are removed, as depicted in Fig.~\ref{fig:subst}. However, this equality remains only spatial, not computational. 

As we already said in Section~\ref{sec:wavelets_cbdam}, it cannot be really claimed that our method performs the~lifting - during the~first bottom-top pass, it does not use any prediction or update operator. However, while slightly simplifying the~reality, the~averaging of four neighboring pixels described in the~beginning of Section~\ref{sec:top-bottom} which is used to construct $\lnorm{i}$ from $\lnorm{i+1}$ can be viewed as an analogy of applying the~update operator of lifting in C-CDAM. The~crucial difference is that in our method, the~lifting is not complete as no prediction operator is applied there to produce the~high-pass part - it computes no residuals yet. In C-BDAM, the~lifting is complete - a~prediction operator is applied there too in order to calculate intermediate quantized residuals. However, reconstructing the~data from the~coarsest LOD to the~finest one in the~subsequent top-bottom pass using just these residuals inside the~inverse lifting equations would not ensure the~required per-sample satisfaction of the~maximum absolute error bound, even if these residuals have been quantized with a~uniform quantizer respecting this error bound. The~reason is that the~calculations inside the~proper lifting are much more intricate. After each step of the~reconstruction, the~maximum deviation from the~target LODs would increase and would become uncontrollable. 

This is exactly why C-BDAM corrects the~intermediate residuals against the~heights inside the~target LODs computed in the~first bottom-top pass. This correction is done in another top-bottom pass. It can be simply described as follows - we just reconstruct a~certain finer LOD node from its coarser parent and compare it to its corresponding target LOD node produced in the~first bottom-top pass. In the~places where the~reconstruction differs from the~target more than the~maximum-deviation constraint allows, the~corresponding residuals are shifted within the~quantization buckets, so that this constraint becomes satisfied. Because C-BDAM uses the~same quantizer as our method - the~uniform one, set to satisfy the~maximum-error bound - such a~shift is possible to find. To find the~correct number of quantization buckets by which a~residual should be shifted, some intricate computations must be performed, including division, which is undoubtedly a~large performance hit. These computations are straightforwardly derived from the~lifting equations.

After studying all these equations, we saw an~opportunity for simplification there - once it is required to perform a~top-bottom pass to correct the~intermediate residuals in order to satisfy the~maximum error bound constraint, we did not really see it as neccessary to calculate any temporary residuals inside the~lifting of the~bottom-top pass. This is the~reason why we use just an~analogy of the~update operator in the~first bottom-top pass to produce the~target mip-maps and do not compute any high-pass information (residuals) there yet, and so do not utilize any prediction operator there. We just let the~suitable values of residuals be computed in the~following top-bottom pass. These values directly satisfy the~maximum-deviation constraint and thanks to the~fact that we do not need to ensure this with respect to any intricate lifting scheme, it becomes very simple, if not trivial, to compute them (sec.~\ref{sec:top-bottom}). Inside one reconstruction step of the~following top-bottom pass, all we do is predict the~yet unknown heights in the~finer LOD with as much accuracy as possible. These predictions, however, are not linked to the~previously performed bottom-top pass, because they have not been applied there at all. Thus, even though the~prediction operator is applied three times inside one reconstruction step, we do not have to pay any atention for the~equations resulting from it to be exacly inverse to the~ones performed in the~bottom-top pass. Then we compute the~final residuals directly with respect to the~target values calculated in the~first bottom-top pass. This is undoubtedly a~significant deviation from both C-BDAM and the~standard second-generation wavelet scheme. 

All in all, the~way the~residuals are computed in this method is a~great simplification of the~approach used in C-BDAM. Our approach does not even conform to the~wavelet scheme of second generation - the~lifting is incomplete and the~reconstruction is not the~inverse of lifting. However, our opinion is that without the~per-level residuals correction in the~subsequent top-bottom pass, it makes sense to respect this wavelet scheme, because it ensures computational equivalence with the~wavelets of the~first generation. But as soon there comes the~need to correct the~residuals at each level, we think that it starts to make no sense to still conform to this scheme, as these corrections destroy this equivalence at a~glance - once a~quantized residual is manually shifted, so that the~resulting value gets closer to the~target data, it can no longer be claimed that any of the~subsequent reconstruction steps is the~exact inverse to the~corresponding lifting performed in the~first bottom-top pass. Additionally, due to the~deviation of C-BDAM from the~normal update-first wavelet scheme of second generation which has already been discussed in Section~\ref{sec:wavelets_cbdam}, we question whether it is still computationally equivalent with the~wavelets of the~first generation even if it were not for the~residuals quantization or cropping. These are the~reasonss why we suppose that we can optimize the~computations performed inside the~second top-bottom pass without any cost. With respect to all the~discussed matters, this method should be called wavelet-inspired rather than wavelet-based.